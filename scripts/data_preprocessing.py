import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split


# define removing duplicates function
class RemoveDuplicates(BaseEstimator, TransformerMixin):
    '''
    remove duplicate rows from dataset 
    '''
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        X_cleaned = X.drop_duplicates().reset_index(drop=True)
        # print simple logs
        removed_count = len(X) - len(X_cleaned)
        percentage = (removed_count*100)/len(X)
        if removed_count > 0:
            print(f"Removed {removed_count} duplicate rows, {percentage:.2f}% of the dataset")
        else:
            print(f'No duplicate rows detected.')
        return X_cleaned

    
# handle missing values
class HandleMissingValues(BaseEstimator, TransformerMixin):
    """
    Impute missing values in numeric columns using median values.
    """
    def fit(self, X, y=None):
        self.median_ = X.median(numeric_only=True, skipna=True)
        return self

    def transform(self, X):
        missing_before = X.isna().sum()
        df_handled = X.fillna(self.median_)
        replaced_count = missing_before[missing_before > 0]
        if not replaced_count.empty:
            print("Missing value imputation summary:")
            for col, count in replaced_count.items():
                if col in self.median_:
                    print(
                        f"  Column '{col}': {count} values replaced "
                        f"with median = {self.median_[col]}"
                    )
        else:
            print("No missing values detected.")
        return df_handled


# IQR methods
class DetectOutliers(BaseEstimator, TransformerMixin):
    """
    Detect and replace outliers using the IQR method.
    """
    def __init__(self):
         """
        Initialize the outlier detector.
        """
        # store the computation
         self.stats = {}

    def fit(self, X, y=None):
        """
        Compute IQR statistics for numeric columns.

        Parameters
        ----------
        X : pandas.DataFrame
            Input dataset.
        y : None, optional
            Ignored.

        Returns
        -------
        self
        """
        # select the numeric columns
        numeric_col = X.select_dtypes(include=[np.number]).columns.tolist()
        self.process_col = numeric_col
        # compute
        for col in self.process_col:
            replace_value = X[col].median()
            Q1 = X[col].quantile(0.25)
            Q3 = X[col].quantile(0.75)
            IQR = Q3 - Q1
            # define the upper and lower bounds
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            # store
            self.stats[col] = {
                "lower": lower_bound,
                "upper": upper_bound,
                "replace": replace_value
            }

        return self        

    def transform(self, X):
        """
        Replace outliers with the median of each column.

        Parameters
        ----------
        X : pandas.DataFrame
            Input dataset.

        Returns
        -------
        pandas.DataFrame
            DataFrame with outliers replaced.
        """
        X_target = X.copy()

        for col in self.process_col:
            stats = self.stats[col]
            lower = stats["lower"]
            upper = stats["upper"]
            replace_value = stats["replace"]
    
            # marking outliers
            mask = (X_target[col] < lower) | (X_target[col] > upper)
            outlier_count = mask.sum()  # count the outliers

            # print the quantity of the replacement
            if outlier_count > 0:
                print(f"Column '{col}' has {outlier_count} outliers replaced by {replace_value}.")
            # replace outliers
            if mask.any():
                X_target.loc[mask, col] = replace_value
        return X_target


# handle the categorical variables
class EncodeCategoricalVariables(BaseEstimator, TransformerMixin):
    """
    Convert categorical columns to numerical
    """
    def fit(self, X, y=None):
        return self
    
    def transform(self, X):
        # identify categorical columns
        categorical_cols = X.select_dtypes(exclude=['number']).columns
        n_categorical = len(categorical_cols)

        # one-hot encode
        df_encoded = pd.get_dummies(X)

        # count the new columns generated by encoding
        new_cols = X.shape[1] - df_encoded.shape[1]
        # print simple logs
        if n_categorical > 0:
            print(
                f"Detected {n_categorical} categorical columns. "
                f"Generated {new_cols} encoded feature columns."
            )
        else:
            print("No categorical columns detected. No encoding applied.")

        return df_encoded


# creat data preprocessing pipeline
def data_preprocessing_pipeline():
    """
    Create a complete data preprocessing pipeline.

    The pipeline performs the following steps in order:
    1. Remove duplicate rows
    2. Handle missing values using median imputation
    3. Detect and replace outliers using the IQR method
    4. Encode categorical variables using one-hot encoding

    Returns
    -------
    sklearn.pipeline.Pipeline
        A scikit-learn Pipeline object for data preprocessing.
    """
    pipeline = Pipeline([
         ("remove_duplicates", RemoveDuplicates()),
         ("handle_missing_values", HandleMissingValues()),
         ("detect_outliers", DetectOutliers()),
         ("encode_categorical_variables", EncodeCategoricalVariables()),
         ("standscaler", StandardScaler())
    ])
    return pipeline

def split_set(data, test_size=0.15, val_size=0.15, seed=42):
    """
    Split data into train, validation, and test sets.

    Assumes the target column is named 'output' and returns
    feature matrices and label vectors for each split.
    """
    temp_size = test_size + val_size
    temp_split_size = val_size / (temp_size)
    # split to training set and temporary set
    train_set, temp_set = train_test_split(data, test_size=temp_size, random_state=seed)
    # split to validation set and test set
    val_set, test_set = train_test_split(temp_set, test_size=temp_split_size, random_state=seed)
    # split features and label
    X_train_split = train_set.drop(columns=['output'])
    y_train = train_set["output"]
    X_val_split = val_set.drop(columns=['output'])
    y_val = val_set['output']
    X_test_split = test_set.drop(columns=['output'])
    y_test = test_set['output']

    return X_train_split, X_val_split, X_test_split, y_train, y_val, y_test


def process_data(data):
    """
    Split data and apply a preprocessing pipeline.

    The pipeline is fitted on the training set and applied
    to validation and test sets.
    """
    X_train_split, X_val_split, X_test_split, y_train, y_val, y_test=split_set(data)

    pipeline = data_preprocessing_pipeline()
    print("Training set:")
    X_train = pipeline.fit_transform(X_train_split)
    print("Validation set:")
    X_val = pipeline.transform(pd.DataFrame(X_val_split))
    print("Test set:")
    X_test = pipeline.transform(pd.DataFrame(X_test_split))

    return X_train, X_val, X_test, y_train, y_val, y_test


if __name__ == '__main__':
    pass